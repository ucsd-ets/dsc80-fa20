{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSC 80: Lab 06\n",
    "\n",
    "### Due Date: Saturday November 14th, 11:59 PM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "Much like in DSC 10, this Jupyter Notebook contains the statements of the problems and provides code and markdown cells to display your answers to the problems. Unlike DSC 10, the notebook is *only* for displaying a readable version of your final answers. The coding work will be developed in an accompanying `lab*.py` file, that will be imported into the current notebook.\n",
    "\n",
    "Labs and programming assignments will be graded in (at most) two ways:\n",
    "1. The functions and classes in the accompanying python file will be tested (a la DSC 20),\n",
    "2. The notebook will be graded (for graphs and free response questions).\n",
    "\n",
    "**Do not change the function names in the `*.py` file**\n",
    "- The functions in the `*.py` file are how your assignment is graded, and they are graded by their name. The dictionary at the end of the file (`GRADED FUNCTIONS`) contains the \"grading list\". The final function in the file allows your doctests to check that all the necessary functions exist.\n",
    "- If you changed something you weren't supposed to, just use git to revert!\n",
    "\n",
    "**Tips for working in the Notebook**:\n",
    "- The notebooks serve to present you the questions and give you a place to present your results for later review.\n",
    "- The notebook on *lab assignments* are not graded (only the `.py` file).\n",
    "- Notebooks for PAs will serve as a final report for the assignment, and contain conclusions and answers to open ended questions that are graded.\n",
    "- The notebook serves as a nice environment for 'pre-development' and experimentation before designing your function in your `.py` file.\n",
    "\n",
    "**Tips for developing in the .py file**:\n",
    "- Do not change the function names in the starter code; grading is done using these function names.\n",
    "- Do not change the docstrings in the functions. These are there to tell you if your work is on the right track!\n",
    "- You are encouraged to write your own additional functions to solve the lab! \n",
    "    - Developing in python usually consists of larger files, with many short functions.\n",
    "    - You may write your other functions in an additional `.py` file that you import in `lab**.py` (much like we do in the notebook).\n",
    "- Always document your code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing code from `lab**.py`\n",
    "\n",
    "* We import our `.py` file that's contained in the same directory as this notebook.\n",
    "* We use the `autoreload` notebook extension to make changes to our `lab**.py` file immediately available in our notebook. Without this extension, we would need to restart the notebook kernel to see any changes to `lab**.py` in the notebook.\n",
    "    - `autoreload` is necessary because, upon import, `lab**.py` is compiled to bytecode (in the directory `__pycache__`). Subsequent imports of `lab**` merely import the existing compiled python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lab06 as lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic HTML tags practice\n",
    "\n",
    "**Question 1**\n",
    "\n",
    "Create a very basic `html` file that satisfies the following properties:\n",
    "\n",
    "1. Has `<head>` and `<body>` tags.\n",
    "2. Has a title\n",
    "3. Inside the body tags:\n",
    "    * At least two headers\n",
    "    * At least three images:\n",
    "        * At least one image must be a local file;\n",
    "        * At least one image must be linked to online source; \n",
    "        * At least one image has to have default text when it cannot be displayed.\n",
    "    * At least three references (hyperlinks) to different web pages;\n",
    "    * At least one table with two columns.\n",
    "    \n",
    "        \n",
    "   \n",
    "4. Save your work as `lab06_1.html` in the same directory as `lab06.py`, make sure it loads in the browser and do not forget to submit it.\n",
    "5. **Do not forget to submit all data files needed to display your page.**\n",
    "\n",
    "**Note:** You can toy with (basic) HTML in the cells of a notebook, using either a \"markdown cell\" or by using the `IPython.display.HTML` function. However, be sure to open your saved file in a browser to be sure the page displays properly!\n",
    "\n",
    "**Note:** If you work within Jupyter Notebook, you can later copy your text into a text editor and save it with the .html extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping an Online Bookstore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2**\n",
    "\n",
    "Browse through the following fake on-line bookstore: http://books.toscrape.com/. This website is meant for toying with scraping.\n",
    "\n",
    "Scrape the website, collecting data on all books that have **at least a four-star rating**, with a price **under £50** and belong to the book categories you want. You should collect the data in a dataframe as below (if you get an encoding error on your prices columns, like you see in the table below, don't worry about it):\n",
    "<img src=\"bookdata.png\">\n",
    "\n",
    "\n",
    "Do this using the following steps:\n",
    "1. Create a function `extract_book_links` that takes in the content of a book-listing page (a string of html), and returns a list of urls of book-detail pages that satisfy the requirements on \"*at least* a four-star rating, and prices are *under* £50\". \n",
    "\n",
    "2. Create a function `get_product_info` that takes in the content of a book-detail page (a string of html), a variable `categories` that is a list of book categories you want. If this input book is in the categories you want, returns a dictionary corresponding to a row in the dataframe in the image above (where the keys are the column names and the values are the row values); else, skip this book since this is not the book you want (ie. return None).\n",
    "\n",
    "3. Create a function `scrape_books` of a single variable `k` that scrapes the first `k` pages of the bookstore (as determined by starting at the url above and clicking on the 'next' button),a variable `categories` that is a list of book categories you want, and returns a dataframe of books as the picture above. (Note: make sure the books returned satisfy the requirements set in part 1 about rating and price).\n",
    "\n",
    "\n",
    "*Note:* Your function should take under 180 seconds to run through the entire bookstore.\n",
    "\n",
    "*Note:* Don't worry about type casting (ie changing number of reviews to an int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = os.path.join('data', 'products.html')\n",
    "text = open(fp, encoding='utf-8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API Requests\n",
    "**Question 3**\n",
    "\n",
    "You trade stocks as a hobby. As an avid pandas coder, you figured it is best to calculate some statistics by pulling data from a public API (https://financialmodelingprep.com/developer/docs/#Stock-Historical-Price). Specifically, \"Historical price with change and volume interval\".\n",
    "\n",
    "Some definitions (these are the ones you need to know):\n",
    "- open: The opening price of a stock at the beginning of a trading day\n",
    "- close: The closing price of a stock at the end of a trading day\n",
    "- volume: The total number of shares being traded in a day\n",
    "- percent change: difference in price with respect to the original price (in percentages)\n",
    "\n",
    "\n",
    "1. Create a function `stock_history` which takes in the stock code (`ticker`) as a string, `year` and `month` as integers, and return a dataframe which has the price history for that stock in that month (include all columns).\n",
    "\n",
    "2. Create a function `stock_stats` that takes in the output dataframe from `stock_history` and output the stock price change as a percentage and a rough total transaction volume **in billion dollars** for that month. Assume that on average, shares are traded at the midpoint price of high and low for that day. Return these two values as a tuple in a readable format: reserve 2 decimal points for both values and add a plus or minus sign at the front of the percent change. \n",
    "$$ \\text{Total Transaction Volume (in dollars)} = \\text{Volume (number of shares traded)} \\times \\text{Price} $$\n",
    "\n",
    "*Example*: If \\\\$BYND opens at \\\\$80 and closes at \\\\$120 with a volume of 1 million, its percent change for the day is $(\\$120-\\$80) \\div \\$80 = +50.00\\%$. And the estimated total transaction volume is: $(\\$80+\\$120) / 2 \\times 10^6 = 0.10\\text{B}$.\n",
    "\n",
    "\n",
    "Hint: [pd.date_range](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.date_range.html), \n",
    "\n",
    "*Note:* Make sure you read the API documentation if you get stuck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_endpoint = 'https://financialmodelingprep.com/api/v3/historical-price-full/{}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comment Threads\n",
    "\n",
    "**Question 4**\n",
    "\n",
    "As a hacker, you get your daily dose of tech news on [Hacker News](https://news.ycombinator.com/). The problem now is that you don't have internet access on your phone in your morning commute to work, so you want to save the interesting stories' comments thread beforehand in a flat file source like csv. You find their API documentation ( https://github.com/HackerNews/API) and implement the following task:\n",
    "\n",
    "1. Write a function `get_comments` that takes `storyid` as a parameter and returns a dataframe of all the comments below the news story. You can ignore 'dead' comments (you will know it when you see it). **Make sure the order of the comments in your dataframe is from top to bottom just as you see on the website**. You are allowed to use loops in this function. Addtional requirement: write at least one helper method\n",
    "\n",
    "You only want these information for the comments:\n",
    "1. `id`: the unique ids\n",
    "2. `by`: the author of the comment\n",
    "3. `parent`: who (also in unique ids) they are replying to\n",
    "4. `text`: the actual comment\n",
    "5. `time`: when the comment is created (in `pd.datetime` format)\n",
    "\n",
    "Hints:\n",
    "1. Use depth-first-search when traversing the comments tree.\n",
    "2. https://docs.python.org/3/tutorial/datastructures.html#using-lists-as-stacks.\n",
    "3. Check the size of your dataframe to the story's `descendants` attribute (number of comments)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_endpoint = \"https://hacker-news.firebaseio.com/v0/item/{}.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations! You're done!\n",
    "\n",
    "* Submit the lab on Gradescope"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
