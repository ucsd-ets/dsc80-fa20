{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 04: Language Models\n",
    "\n",
    "## Checkpoint Due Date (Questions 1-5): Monday November 23, 11:59 PM\n",
    "\n",
    "## Due Date:  Monday November 30, 11:59 PM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checkpoint Instructions\n",
    "* The checkpoint requires you to turn in questions 1-5.\n",
    "* The checkpoint will be graded for approximate correctness: easier than the final tests; harder than the doctests.\n",
    "* The checkpoint will count toward the extra-credit grade at the end of the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Language Models\n",
    "\n",
    "In this project, you will build *[statistical language models](https://en.wikipedia.org/wiki/Language_model)* using public domain books found on [Project Gutenberg](https://www.gutenberg.org/). Language models attempt to capture the likelihood that a given sequence of words occur in a given \"language\" (the precise term is \"corpus\" or \"corpora\"). Here, \"language\" is a broad term that, in addition to the normal usage, may mean the language of a particular author or style. Applications of language models are found in areas such as language generation, authorship attribution, auto-complete / word-suggestion, and machine translation.\n",
    "\n",
    "In this project, you will learn about a simple but effective type of language model (LM) called N-gram models. Given a sentence (that is, a sequence of words $w = w_1\\ldots w_n$), you would like to understand the probability that sentence is used: \n",
    "$$P(w) = P(w_1,\\ldots,w_n)$$\n",
    "However, sentences are built from words, and the likelihood that a word occurs where it does, depends on the words it follows. This points to using *conditional probability* to understand $P(w)$. That is, we can write:\n",
    "\n",
    "$$\n",
    "P(w) = P(w_1,\\ldots,w_n) = P(w_1) \\cdot P(w_2|w_1) \\cdot P(w_3|w_1,w_2) \\cdot\\ldots\\cdot P(w_n|w_1,\\ldots,w_{n-1})\n",
    "$$  \n",
    "This is also called the *chain rule* for probabilities.  \n",
    "**Example:** Consider the sentence $w = $ `when I eat pizza, I wipe off the grease.`\n",
    "\n",
    "$$\n",
    "P(\\mbox{when}) \\cdot P(\\mbox{I | when}) \\cdot P(\\mbox{ eat | when I})\\cdot P(\\mbox{ pizza | when I eat}) \\cdot \\ldots \\cdot P(\\mbox{ grease | when I eat pizza, I wipe off the})\n",
    "$$\n",
    "\n",
    "That is, the probability that the sentence occurs is the product of the probability that each subsequent word follows the words that came before. For example, the probability $P(\\mbox{ pizza | when I eat})$ is pretty high, as pizza is something that you eat; thus, this term would contribute to this sentence having a higher probability of occurrence.\n",
    "\n",
    "### N-gram language models\n",
    "\n",
    "In the description above, the probability that any word in the sentence (e.g. 'grease') depends on *every* word that preceded it (e.g. 'when'). However, it's often the case the likelihood a word appears in a sentence is influenced more by *nearby* words. \n",
    "* N-gram language models capture this concept that only nearby words matter; they assume the probability a word occurs only depends on the previous $(N−1)$ words. \n",
    "* That is, an N-gram model says: $P(w_n|w_1,\\ldots,w_{n-1}) = P(w_n|w_{n-(N-1)},\\ldots,w_{n-1})$.\n",
    "\n",
    "**Example: trigram model**\n",
    "\n",
    "Consider the sentence $w = $ `when I eat pizza, I wipe off the grease.`\n",
    "\n",
    "$$\n",
    "P(\\mbox{when}) \\cdot P(\\mbox{I | when}) \\cdot P(\\mbox{ eat | when I})\\cdot P(\\mbox{ pizza | I eat}) \\cdot \\ldots \\cdot P(\\mbox{ the | wipe off}) \\cdot P(\\mbox{ grease | off the})\n",
    "$$\n",
    "In this example, we see the trigram model doesn't consider the beginning of the sentence when considering the likelihood of the end word 'grease'.\n",
    "\n",
    "**Definition:** Given the descriptions and definitions above, we define the **N-grams of a text** to be the list of sliding windows of length N.\n",
    "\n",
    "For Example, the trigrams of the sentence $w$ in the example above are:\n",
    "```\n",
    "[('when', 'I', 'eat'), ('I', 'eat', 'pizza'), ... , ('wipe', 'off', 'the'), ('off', 'the', 'grease')]\n",
    "```\n",
    "\n",
    "### Computing an N-gram language model\n",
    "\n",
    "N-gram language models are a collection of conditional probabilities for sequences of words of length $N$. For example, in a trigram model, for every 3-word sequence $w_1, w_2, w_3$, you need to compute:\n",
    "$$P(w_1,w_2,w_3) = P(w_3 | w_1, w_2) = \\frac{C(w_1, w_2, w_3)}{C(w_1, w_2)}$$\n",
    "\n",
    "Where $C(w_1, w_2, w_3)$ is the number of occurrences of the trigram-sequence $w_1, w_2, w_3$ in the dataset and $C(w_1, w_2)$ is the number of occurrences of the bigram-sequence  $w_1, w_2$ in the dataset.\n",
    "\n",
    "For a general N-gram model, the conditional probabilities are computed by dividing the counts of N-grams by the (N-1)-grams they follow.\n",
    "\n",
    "\n",
    "### Tokenizing corpora\n",
    "\n",
    "Computing the probabilities of a language model from a book requires breaking up the text of book into sequences of words. This process is called *tokenization*. In reality though, the sequences are not made up entirely of words, but rather more general terms called *tokens*. In this project tokens will include not only whole words, but also punctuation and other terms. Below are a few examples of other types of tokens:\n",
    "\n",
    "* Punctuation like `,.;'`. For example, the period makes sense as a token, as certain words tend to end sentences (i.e. come before a `.` in an N-gram) and begin sentences (i.e. come after a `.` in an N-gram).\n",
    "* We have special 'START' and 'END' tokens that begin and end every word sequence (in our case, paragraphs of words in a given book). These make sense as tokens, as certain words may tend to begin and end paragraphs. \n",
    "\n",
    "It is useful for the tokens used to represent START and END to be single characters that can never be found in the text of the book you use to create our language model. Thus, you will use two ascii hidden \"[control characters](https://en.wikipedia.org/wiki/C0_and_C1_control_codes#C0_(ASCII_and_derivatives))\":\n",
    "* For START, you will use the character `\\x02` (used for 'beginning of text')\n",
    "* For END, you will use the character `\\x03` (used for 'end of text')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note on checking your work for correctness\n",
    "\n",
    "Building a statistical model involves stringing together difficult-to-follows steps such as data-cleaning and implementing derived mathematical formulas. A fact of life in this line of work is that it can be difficult to assess the correctness of your work. \n",
    "\n",
    "* The Doctests will include checks on the inputs/outputs of your methods.\n",
    "* Doctests will include models on which you can check your work 'by hand'.\n",
    "* Additionally, **you should run your functions on *real* books and assess the reasonableness of your result** as well. Merely passing the doctests is never sufficient to guarantee correctness of your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Outline\n",
    "\n",
    "Below is an outline of the the project. If there are terms you don't understand in following descriptions, you should review the introduction for an explanation of the terms and ideas described.\n",
    "\n",
    "1. Write code to download the text of a book from a url, returning the corpus as a string.\n",
    "1. Tokenize the corpus into sequences of words and paragraphs.\n",
    "1. Create two \"baseline\" langunage models; create text with them using sampling.\n",
    "1. Compute an N-gram language model\n",
    "1. Write code to sample from the N-gram language model; create text with them.\n",
    "1. Evaluate performance (computational) of your language model for different N.\n",
    "1. Evaluate performance (statistical) of your language model for different N.\n",
    "1. Create an auto-completion suggestion model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import project04 as proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the book(s)\n",
    "\n",
    "**Question 1**\n",
    "\n",
    "For this question, you will use `requests` to download and prepare a public domain book from [Project Gutenberg](). Create a function `get_book` that takes in the `url` of a 'Plain Text UTF-8' book and returns a string containing the contents of the book. \n",
    "\n",
    "The function should satisfy the following conditions:\n",
    "* The contents of the book consist of everything between Project Gutenberg's START and END comments.\n",
    "* The contents *will* include title/author/table of contents.\n",
    "* You should also transform any Windows new-lines (`\\r\\n`) with standard new-lines (`\\n`).\n",
    "* You should check the site's `robots.txt` as well and implement a 'pause' in your request in accordance with the website's policy. If the function is called twice in succession, it should not violate the `robots.txt` policy.\n",
    "\n",
    "*Note:* You are encouraged to find whatever books on the website that interest you to test your code and the language models you develop. The text doesn't even need to be an English-language book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the actual book text\n",
    "url = 'http://www.gutenberg.org/files/74/74-0.txt'\n",
    "book_string = proj.get_book(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing the Corpus\n",
    "\n",
    "**Question 2**\n",
    "\n",
    "Now you need to tokenize the corpus in `book_string` (turn the text into a list of tokens -- i.e. words and punctuation). More specifically, create a function `tokenize` that takes in `book_string` and outputs a list of tokens satisfying the following conditions:\n",
    "* The start of any paragraph should be represented in the list with the single character `\\x02` (standing for START).\n",
    "* The end of any paragraph should be represented in the list with the single character `\\x03` (standing for STOP).\n",
    "* Tokens in the sequence of words are split apart at 'word boundaries' (see the regex lecture).\n",
    "* Tokens should include *no* whitespace.\n",
    "* Whitespace (e.g. multiple new-lines) between two paragraphs of text should be ignored.\n",
    "* Two or more newlines count as a paragraph break\n",
    "\n",
    "For example, the following sentence (the ellipses denote preceding/continuing text):\n",
    "```\n",
    "...\n",
    "My phone's dead.\n",
    "\n",
    "I didn't get your call.\n",
    "...\n",
    "```\n",
    "should tokenize to:\n",
    "```\n",
    "[ ...\n",
    "\"My\", \"phone\", \"'\", \"s\", \"dead\", \".\", \"\\x03\", \"\\x02\", \"I\", \"didn\", \"'\", \"t\", \"get\", \"your\", \"call\", \".\"\n",
    "... ]\n",
    "```\n",
    "\n",
    "*Remark:* Your tokenization function should run quickly; you should avoid loops when possible. Your `tokenize` function should run on the the complete works of Shakespeare (in `data/shakespeare`) in **under 10 seconds** to get full credit. Be sure to only request the book once (using your `get_book` function) and save it to file for testing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_fp = os.path.join('data', 'shakespeare.txt')\n",
    "shakespeare = open(shakespeare_fp, encoding='utf-8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = proj.tokenize(book_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.07 s ± 159 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# time your code\n",
    "%timeit proj.tokenize(shakespeare)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Language Model\n",
    "\n",
    "Now that you've learned how to (theoretically) build a language model, you have to implement it in code. Every language model you build will be a class with a few methods in common:\n",
    "\n",
    "* The `__init__` constructor: when you instantiate an LM object, you will need to pass the 'training corpus' on which your model will be trained (i.e. a list of tokens you created above with `tokenize`). The `train` method will then use that data to create a model which is saved in the `mdl` attribute. This code is given to you.\n",
    "* The `train` method takes in a list of tokens (e.g. the output of `tokenize`) and outputs a language model. This language model is usually represented as a `Series` (indexed by tokens; values are probabilities that token occurs), or a `DataFrame`.\n",
    "* The `probability` method takes in a sequence of tokens and returns the probability that this sequence occurs under the language model.\n",
    "* The `sample` method takes in a number `N > 0` and generates a string made up of `N` tokens using the language model. This method generates language.\n",
    "\n",
    "You will create three language model classes in the problems below (Uniform; Unigram; N-gram). For each of these, you will implement each of these methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Models: uniform and unigram LMs\n",
    "\n",
    "### Uniform probability language model\n",
    "\n",
    "**Question 3**\n",
    "\n",
    "A uniform language model is one in which any word is equally likely to appear in any position. The starter code contains a class `UniformLM` which represents a uniform language model. Review the specifics of the class methods above (and in the starter-code) and implement the methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unif = proj.UniformLM(tokens)\n",
    "unif.sample(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram language model\n",
    "\n",
    "**Question 4**\n",
    "\n",
    "A unigram language model is one in which the likelihood a word appears is proportional to its occurrence in the text. That is, it's just the (unconditional) empirical distribution of words in the corpus. The starter code contains a class `UnigramLM` which represents a unigram language model. Review the specifics of the class methods above (and in the starter-code) and implement the methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram = proj.UnigramLM(tokens)\n",
    "unigram.sample(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion: Baseline models\n",
    "\n",
    "You've now trained two baseline language models capable of generating new text from a given training text. Attempt to answer these questions for yourself before you continue.\n",
    "\n",
    "* Which model do you think is better? Why?\n",
    "* What are the ways in which both of these models are bad?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram Model\n",
    "\n",
    "Now you will build an N-gram language model, which we be a little more involved. \n",
    "\n",
    "**Before continuing, go back and read the introduction to understand the concepts described below.**\n",
    "\n",
    "The `NGramLM` class contains a few extra methods and attributes beyond those of `UniformLM` and `UnigramLM`:\n",
    "\n",
    "1. Instantiating `NGramLM` requires both a list of tokens and an `N > 0`, specifying the `N` in N-grams. This parameter is stored in an attribute `N`.\n",
    "1. The `NGramLM` class has a method `create_ngrams` that takes in a list of tokens and returns a list of N-grams (an N-gram is a `tuple` of length `N`). This list of N-grams is then passed to the `train` method to train the N-gram model.\n",
    "1. The `NGramLM` class has an attribute `mdl` that stores all the n-gram language models for `n=1...N`. The smaller n-grams are needed to compute the likelihood a word occurs at the start of a text. This code is included for you in the constructor.\n",
    "1. While the `train` method still creates a language-model (in this case, an N-gram model), this model is most naturally stored as a DataFrame. This DataFrame will have three columns:\n",
    "    - `'ngram'`: containing the n-grams found in the text.\n",
    "    - `'n1gram'`: containing the (n-1)-grams upon which the n-grams in `ngram` are built.\n",
    "    - `'prob'`: containing the probabilities of each n-gram in `ngram`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating n-grams\n",
    "\n",
    "**Question 5**\n",
    "\n",
    "When computing the likelihood that a word occurs in an N-gram model, you compute the probability that word occurs conditional on the (N-1) words that came before it. Therefore, you need to compute a list of N-grams out of your list of tokens (see the introduction for the definition).\n",
    "\n",
    "Create a method of `NGramLM` called `create_ngrams` that takes in a list of tokens and returns a list of N-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the N-gram LM\n",
    "\n",
    "**Question 6**\n",
    "\n",
    "Now, you will compute the probabilities that define N-gram language model itself. Recall that the N-gram LM consists of probabilities\n",
    "$$P(w_1,\\ldots,w_N) = P(w_N | w_1,\\ldots, w_{N-1}) = \\frac{C(w_1, \\ldots, w_N)}{C(w_1,\\ldots, w_{N-1})}$$\n",
    "\n",
    "for every n-gram $(w_1, \\ldots, w_N)$ that occurs in the text.\n",
    "\n",
    "Create a method of `NGramLM` called `train` that takes in a list of N-grams and returns a dataframe of conditional probabilities with the following columns:\n",
    "\n",
    "* `ngram` contains each N-gram that occurs in the text,\n",
    "* `n1gram` contains the (N-1) tokens that precede the last token in the N-gram occupying the same row.\n",
    "* `prob` contains the conditional probabilities associated to the N-gram occupying the same row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing probabilities from the N-gram LM\n",
    "\n",
    "**Question 7**\n",
    "\n",
    "Create a method of `NGramLM` called `probability` that takes in a 'sentence' (sequence of tokens) and returns the likelihood of the sentence under the language model (see the introduction of the formulas).\n",
    "\n",
    "*Remark:* What is the probability of a sentence that contains a 'never before seen' word? What about a 'never before seen' (N-1)-gram? Is this reasonable? (Answer: no, it's not reasonable. Fixing this sort of deficiency is a topic in NLP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling from the N-gram LM\n",
    "\n",
    "**Question 8**\n",
    "\n",
    "As you saw creating the baseline models, sampling from your trained LM provides a way to generate new language. You will code up a `sample` method for `NGramLM` and observe how much better (or worse) the generated language appears when compared to your baseline models.\n",
    "\n",
    "Create a method of `NGramLM` called `sample` that takes in a number `M > 0` and generates a string of M tokens using the language model. It should begin with a starting token $\\texttt{\\\\x02}$, then generate the following words from the probabilities in `self.mdl` and continue picking words conditional on the previous choice. The first $\\texttt{\\\\x02}$ token should not count toward the total length. Helper functions and recursion will be very helpful.\n",
    "\n",
    "*Remark:* If you find, at any point, your LM has no words to sample, then you should return the STOP token (`\\x03`) and continue until you reach the required length.\n",
    "\n",
    "*(Fun) Remark:* Try generating text using different values of `N`. What are the differences (qualitatively)? Don't be alarmed if your samples generate strange quasi-coherent sentences; it's just randomness!\n",
    "\n",
    "*(Fun) Remark:* Try sampling from an `N`-gram model built on books written in different languages (It doesn't even need to be written using the Latin alphabet!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram = proj.NGramLM(3, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ngram.sample(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting the next word in a sentence\n",
    "\n",
    "**OPTIONAL QUESTION**\n",
    "\n",
    "Build a predictor that predicts the most likely word to follow a given sentence. The predictions will be the *maximum likelihood estimate* given by your N-gram model. Recall that your N-gram model contains the probabilities that a token follows an (n-1)-gram; your predictor will pick the token with the highest probability of occurring. \n",
    "\n",
    "Create a function that takes in an instantiated `NGramLM` object and a list of tokens, and predicts the *most likely token* to follow the list of tokens in the input (according to `NGramLM`).\n",
    "\n",
    "*Remark:* For which N does this predictor work best? (Determine this by getting a feel for the output, we won't precisely answer this question).\n",
    "\n",
    "*Remark:* What would this function look like if it were to take in `UniformLM` or `UnigramLM`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance Evaluation\n",
    "\n",
    "**OPTIONAL QUESTION**\n",
    "\n",
    "This question attempts to answer the question \"which choice of `N` is best when creating an N-gram model?\"\n",
    "\n",
    "As you likely noticed when generating text using your N-gram models, the sentences appear to become more coherent as N increases. The naive conclusion would be to make N very large. However, such a choice has a downside that you will investigate.\n",
    "\n",
    "One concern is that as N gets larger, the distribution of N-grams get more *sparse* -- i.e. the collection of n-grams all have counts close to 1. This makes the language model more likely to merely generate text that was already present in the original text, as opposed to generating *new* language (why?). \n",
    "\n",
    "In this question, you will quantify this observation: given an N-gram model and a sample of length M, what percentage of (N+1)-grams in the sample are present in the text that was used to create the model?\n",
    "\n",
    "Create a function which takes in a list of tokens and:\n",
    "1. Generates N-gram models for `N=2,3,4,5,6,7,8` from the given list of tokens,\n",
    "2. Creates samples of length 1000 from each model (remove START/STOP tokens from the samples),\n",
    "3. Calculates the proportion of (N+1)-grams in each sample that were already present in the original list of tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
