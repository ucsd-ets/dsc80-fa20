{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion 5\n",
    "\n",
    "### Due Saturday November 7, 11:59:59PM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Lecture Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import disc05 as disc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture Review: Missing Data\n",
    "\n",
    "### Understanding Why Our Data are Missing\n",
    "\n",
    "It is important to assess why our data are missing, as the observed data may not be representative of the complete dataset, nor the process that generated it. This limits the ability to draw conclusions about the process from the dataset.\n",
    "\n",
    "#### Conditionally Ignorable / Missing at Random (MAR)\n",
    "\n",
    "* Missing at Random means that the tendency for a data point to be missing *explainable* by some of the observed data.\n",
    "    * Whether or not someone answered #1351 on your survey has nothing to do with the missing value (what the answer would've been), but it does have to do with the values of some other variable (e.g. whether it came near the beginning or end of a very long survey).\n",
    "    * Think of this as (Conditionally) Missing at Random because the missingness is conditional on another observed variable.  \n",
    "    * The idea is, if we can control for this conditional variable, we can get an unconditional random subset.\n",
    "    \n",
    "$$Pr({\\rm data\\ is\\ present\\ } | Y_{obs}, Y_{mis}, \\psi) = Pr({\\rm data\\ is\\ present\\ } |\\ Y_{obs}, \\psi)$$\n",
    "\n",
    "#### Unconditionally Ignorable / Missing Completely at Random (MCAR)\n",
    "\n",
    "* The fact that a certain value is missing has nothing to do with its hypothetical value and with the values of other variables.\n",
    "    * There is no relationship between whether a data point is missing and any values in the data set, missing or observed.\n",
    "\n",
    "    * The missing data are just a random subset of the data.\n",
    "    \n",
    "$$Pr({\\rm data\\ is\\ present\\ } | Y_{obs}, Y_{mis}, \\psi) = Pr({\\rm data\\ is\\ present\\ } |\\ \\psi)$$\n",
    "\n",
    "#### Non-Ignorable / Missing Not at Random (NMAR)\n",
    "\n",
    "* The missing value depends on the hypothetical (unobserved) value itself and isn't the tendency to be missing is'not explainable by any other data. \n",
    "    * e.g. People with high salaries generally do not want to reveal their incomes in surveys.\n",
    "* Because non-ignorable missing data depends on the values of the unobserved data, you cannot determine it from the data alone! It is an assumption on the data generating process.\n",
    "* If the you can add more attributes that 'explain' the missingness, non-ignorable data can become ignorable.\n",
    "    * e.g. Adding Education level and zip code may explain the non-response of people with high-salaries.\n",
    "    \n",
    "$$Pr({\\rm data\\ is\\ present\\ }| Y_{obs}, Y_{mis}, \\psi)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining Missingness Types\n",
    "\n",
    "Determining the mechanisms of missingness involves *searching for patterns to explain how data are missing*. When assessing a dataset:\n",
    "1. In what ways might the missing values in a dataset come to be missing? Try to come up with explanations involving the data generating process. If you arrive at reasons for missingness that are not explainable by the dataset, then you have identified *non-ignorable* missing data. If this is the case, can you think attributes to collect that might explain the missing data?\n",
    "1. Once you believe that patterns in the missingness of an attribute are explainable by the observed data, you can try to determine the nature of these patterns. In this case, look at the data when the attribute in question is missing, versus when it's not missing: do the data look the the same in both cases? To do this:\n",
    "    - Consider the `missing` and `not_missing` data as two groups, and compare the distributions of the other attributes to determine if they look similar.\n",
    "    - To determine the similarity of the two distributions, use a permutation test.\n",
    "1. If the two distributions are different, then you've identified a pattern in the missingness: the likelihood the attribute is missing depends on the values of another attribute in the dataset (it's conditional on the other attribute).\n",
    "1. If there are no patterns in the missingness of the data, then the dataset is *unconditionally ignorable*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples: Salaries\n",
    "\n",
    "Below are the salaries of San Diego city employees, with missing data. Assuming that this data was collected via a survey given at work. (In reality, this dataset is a *census*, including every employees salaries).\n",
    "\n",
    "**Question:** What are some potential explanations for why salary data might be missing from this dataset? What are potential fields that could be collected to explain the missingness?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salaries1 = pd.read_csv('data/salaries_miss1.csv')\n",
    "salaries2 = pd.read_csv('data/salaries_miss2.csv')\n",
    "salaries1[['Total Pay']].head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll assess whether the 'Total Pay' is missing conditional on job 'Status'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = salaries2\n",
    "# df = salaries1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().mean().rename('proportion null').to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the empirical distribution of 'Status' conditional on missingness of 'Total Pay'. Do these distributions look similar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distr = (\n",
    "    df\n",
    "    .assign(is_null=df['Total Pay'].isnull())\n",
    "    .pivot_table(index='Status', columns='is_null', aggfunc='size')\n",
    "    .apply(lambda x:x/x.sum())\n",
    ")\n",
    "\n",
    "distr.plot(kind='bar', title='Job Status conditional on missingness of Pay');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this difference between the distributions due to chance? Or are the distributions significantly different?\n",
    "\n",
    "1. Measure similarity between the *observed* distributions with the TVD.\n",
    "1. Run a permutation test to assess the magnitude of the observed statistic. Repeatedly:\n",
    "    - The two groups are *missing pay* and *not missing pay*.\n",
    "    - Shuffle the labels, separate the shuffled groups into 'random groups'.\n",
    "    - Calculate the tvd of the random groups.\n",
    "1. The distribution of the 'shuffled TVDs' is the null distribution. How likely was the observed TVD generated from the null distribution? Calculate a p-value!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observed TVD\n",
    "obs = distr.diff(axis=1).abs().iloc[:,-1].sum() / 2\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle once\n",
    "(\n",
    "    df\n",
    "    .assign(is_null=df['Total Pay'].isnull())  # create label column\n",
    "    .assign(Status=df['Total Pay'].sample(frac=1, replace=False).reset_index(drop=True)) # shuffle 'total pay' -- same effect as shuffling label\n",
    "    .pivot_table(index='Status', columns='is_null', aggfunc='size')  # compute counts for the conditional distribution\n",
    "    .apply(lambda x:x/x.sum())  # normalize counts to a distribution\n",
    "    .diff(axis=1).abs().iloc[:,-1].sum() / 2    # calculate the TVD of the shuffled distributions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "\n",
    "tvds = []\n",
    "for _ in range(N):\n",
    "    shuffed = (\n",
    "        df\n",
    "        .assign(is_null=df['Total Pay'].isnull())\n",
    "        .assign(Status=df['Total Pay'].sample(frac=1, replace=False).reset_index(drop=True))\n",
    "        .pivot_table(index='Status', columns='is_null', aggfunc='size')\n",
    "        .apply(lambda x:x/x.sum())\n",
    "        .diff(axis=1).abs().iloc[:,-1].sum() / 2\n",
    "    )\n",
    "    \n",
    "    tvds.append(shuffed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(tvds).plot(kind='hist', title='permutation test for missingness of Pay dependent on Status')\n",
    "plt.scatter(obs, 0, c='r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Does the observed dataset well-represent, over-represent, or under-represent the mean salary of city employees?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distr.plot(kind='bar', title='Distribution of Status conditional on missingness of Pay');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Total Pay'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** How could you estimate the mean of the population with this understanding of missingness?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the population mean from the complete dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salaries = pd.read_csv('data/salaries.csv')\n",
    "salaries['Total Pay'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What does this have to do with imputation? What if we want an imputed dataset with approximately the same mean as the population mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Working with Missing Data in Pandas\n",
    "\n",
    "* The way in which `Pandas` handles missing values is constrained by its reliance on the `NumPy` package, which does not have a built-in notion of `NaN` values for non-floating-point data types.\n",
    "    * `Pandas` chose to use sentinels for missing data, and further chose to use two already-existing Python null values: the special floating-point `NaN` value, and the Python `None` object. \n",
    "    * This choice has some side effects, as we will see, but in practice ends up being a good compromise in most cases of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `None`: Pythonic Missing Data\n",
    "\n",
    "* The first sentinel value used by Pandas is `None`, a Python singleton object that is often used for missing data in Python code. \n",
    "    * Because it is a Python object, `None` cannot be used in any arbitrary `NumPy`/`Pandas` array, but only in arrays with data type 'object' (i.e., arrays of Python objects)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals1 = np.array([1, None, 3, 4])\n",
    "vals1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The use of Python objects in an array also means that if you perform aggregations like sum() or min() across an array with a None value, you will generally get an error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this reflects the fact that addition between an integer and None is undefined\n",
    "vals1.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `NaN`: Missing Numerical Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The other missing data representation, `NaN` (acronym for Not a Number), is different; it is a special floating-point value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals2 = np.array([1, np.nan, 3, 4]) \n",
    "vals2.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Notice that `NumPy` chose a native floating-point type for this array: this means that unlike the object array from before, this array supports fast operations pushed into compiled code. \n",
    "* You should be aware that `NaN` is a bit like a data virus in that it infects any other object it touches. \n",
    "    * Regardless of the operation, the result of arithmetic with `NaN` will be another `NaN`.\n",
    "    * Comparisons with `NaN` always return false.\n",
    "    * Use the `pd.isnull` function to check if a single value is `NaN`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 + np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0 *  np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nan == np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nan > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nan <= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.isnull(np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This means that aggregates over the values are well defined (i.e., they don't result in an error) but not always useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals2.sum(), vals2.min(), vals2.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `NaN` and `None` in Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `NaN` and `None` both have their place, and `Pandas` is built to handle the two of them nearly interchangeably, converting between them where appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series([1, np.nan, 2, None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For types that don't have an available sentinel value, Pandas automatically type-casts when NA values are present. \n",
    "    * For example, if we set a value in an integer array to `np.nan`, it will automatically be upcast to a floating-point type to accommodate the NA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.Series(range(2), dtype=int)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0] = None\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying Pandas Methods to Columns with Null Values\n",
    "* Unlike `NumPy`, methods applied to Pandas Series/DataFrame usually silently apply `dropna` before applying the operation.\n",
    "* Below, compare the equivalent `Pandas` and `Numpy` methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Pandas method `mean` to column 'B' (a Series)\n",
    "df.loc[:, 'B'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Numpy method `mean` to the np.ndarray underlying column 'B'.\n",
    "df.loc[:, 'B'].values.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion Question**\n",
    "* What is the output of `df.loc[:, 'C'].values.min()`?\n",
    "* What is the output of  `df.loc[:, 'C'].min()`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df\n",
    "# ans1 = df.loc[:, 'C'].values.min()\n",
    "# ans2 = df.loc[:, 'C'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputing (Filling) Null Values\n",
    "\n",
    "* Sometimes rather than dropping NA values, we'd rather replace them with a valid value. \n",
    "* This value might be a single number like zero, or it might be some sort of imputation or interpolation from the good, non-null values. \n",
    "    * You could do this in-place using the `isnull()` method as a mask, but because it is such a common operation `Pandas` provides the `fillna()` method, which returns a copy of the array with the null values replaced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.Series([1, np.nan, 2, None, 3], index=list('abcde'))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(data.fillna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward-fill\n",
    "data.fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# back-fill\n",
    "data.fillna(method='bfill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_dict = dict(zip(list('abcde'), [1, 1.5, 2, 2.5, 3]))\n",
    "fill_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.fillna(fill_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_values =  pd.Series([1, 1.5, 2, 2.5, 3], index=list('abcde'))\n",
    "fill_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.fillna(fill_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For `DataFrames`, the options are similar, but needs additional data specified:\n",
    "    1. Either an axis along which the fills take place, (for `method` options).\n",
    "    2. Keys for which column to which to apply each dictionary/series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# if a previous value is not available during a forward fill, the NA value remains\n",
    "salaries2.fillna(method='ffill', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill columns 0 and 3 with column 2; Fill column 1 with [10,20,30]\n",
    "fill_dict = {\n",
    "    'B': df.iloc[:, 3],\n",
    "    'C': df.iloc[:, 4],\n",
    "'D': pd.Series({13: 538, 15: 270})\n",
    "}\n",
    "df.fillna(fill_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Practice Problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg = pd.read_csv('data.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the proportion of null values per column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1**\n",
    "\n",
    "Given the dataframe `dg`, impute (i.e. fill-in) the missing values of column `B` with the value of the index for that row. To do this, write a function `impute_with_index` that takes in a dataframe `dg` and outputs the imputed column `B` (a Series).\n",
    "\n",
    "*Remark*: can you fill in *all* the columns this way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2**\n",
    "\n",
    "Given the dataframe `dg`, impute (i.e. fill-in) the missing values of each column using the last digit of the value of column `A`. For example, if a missing value for a given row has value 78 in column `A`, then fill in the missing value with the number 8.\n",
    "\n",
    "Write your answer as a function `impute_with_digit` that takes in a dataframe `dg` and outputs the imputed dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
