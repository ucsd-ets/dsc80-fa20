{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion 9\n",
    "\n",
    "### Due Saturday Dec 5th, 11:59:59PM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-Learn: Transformers, Estimators, and Pipelines\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers and Estimators\n",
    "\n",
    "Scikit learn includes two *base* modeling classes: Transfomers and Estimators. Both are classes that are meant to be *fit* on (training) data and then later used to transform (or predict with) unseen data.\n",
    "\n",
    "### Transformers\n",
    "\n",
    "* Transformers take input data and transform it into output data via the `transform` method.\n",
    "* Sometimes transformers need prior information (parameters) about the data before transforming it.\n",
    "    - In this case, the transformer is *fit* using the `fit` method on training data to estimate the parameters.\n",
    "    - Once fit, the transformer may then be applied to `test` data (or unseen, new data).\n",
    "* Fit parameters are accessed via an instance variable that ends in an *underscore*.\n",
    "\n",
    "**Question 1** Using `Binarizer`, transform the `city-mpg` and `highway-mpg` column to 0 if the mpg is less than or equal to 25 and 1 if it's greater than 25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cars = pd.read_csv('cars.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Binarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2** Using `FunctionTransformer`, transform the `city-mpg` and `highway-mpg` columns to a log-scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most transformers you will use will require being *fit to training data* before using it. An example of this is one-hot-encoding: before applying one-hot encoding to a column, you must determine the number of distinct values in the column (as that number determines the number of columns in the output).\n",
    "\n",
    "**Question 3** *(Fit transformers properly handle unseen values)*\n",
    "\n",
    "1. One-hot encode the `body-style` column using `OneHotEncoder`. What is the dimension of the output? Which column corresponds to which value of `body-style`? (If you can't remember the attribute name, look up the documentation!)\n",
    "1. Fit a `OneHotEncoder` on the *first 5 rows* of the `body-style` column. Use this 'training data' to one-hot-encode the `body-style` in rest of the dataset. Why does it throw an exception? Look at the documentation -- what is the relevant parameter to avoid this? What are the implications of setting this parameter? What is the dimension of the output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you observed, the categories of `OneHotEncoder` are learned from training data and saved as an attribute of the transformer. These categories are then used to transform new, unseen data which is then used by other pieces of the ML pipeline.\n",
    "\n",
    "Below is an illustration of why *fitting* a transformer is so important:\n",
    "\n",
    "*The dangers of `pd.get_dummies`*: Pandas offers it's own one-hot encoder called `get_dummies`. This function is stateless; every time it's called, it determines the categories of the input data and one-hot encodes that data using those categories. However, as you saw in the above question, this is not a realistic use of the function!\n",
    "\n",
    "To illustrate this:\n",
    "1. We will create a one-hot encoding using scikit-learn that we will pass into a linear regression model.\n",
    "1. We will create a *stateless* one-hot encoder that we will pass into a linear regression model.\n",
    "\n",
    "Both of these models will be trained on the first 5 rows of the dataset; the rest will be used as 'unseen' data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train = cars[['body-style']].head(5), cars['price'].head(5)\n",
    "X_test, y_test = cars[['body-style']].tail(-5), cars['price'].tail(-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using sklearn transformers. What are the categories of the OneHotEncoder?\n",
    "ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "lr = LinearRegression()\n",
    "\n",
    "ohe.fit(X_train)\n",
    "features = ohe.transform(X_train)\n",
    "lr.fit(features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict on the new data using the fit model:\n",
    "lr.predict(ohe.transform(X_test))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# using a stateless one-hot encoder.\n",
    "ohe = FunctionTransformer(pd.get_dummies, validate=False)\n",
    "lr = LinearRegression()\n",
    "\n",
    "ohe.fit(X_train)\n",
    "features = ohe.transform(X_train)\n",
    "lr.fit(features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# debug this!\n",
    "lr.predict(ohe.transform(X_test))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Even worse, there are cases where such an ML pipeline doesn't even throw an exception -- the incorrect columns get silently passed on. \n",
    "\n",
    "**The below predictions are wrong! Can you tell that it's wrong?** Debugging statistical output is notoriously hard -- this is why statistical analysis of the output data is always an important step to check your work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train = cars[['body-style']].head(5), cars['price'].head(5)\n",
    "X_test, y_test = cars[['body-style']].iloc[5:20], cars['price'].iloc[5:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ohe = FunctionTransformer(pd.get_dummies, validate=False)\n",
    "lr = LinearRegression()\n",
    "\n",
    "ohe.fit(X_train)\n",
    "features = ohe.transform(X_train)\n",
    "lr.fit(features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# THIS IS WRONG! debug this! \n",
    "lr.predict(ohe.transform(X_test))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building custom Transformers\n",
    "\n",
    "Building your own transformer class is easy! There is are convenient base-classes you can inherit: `TranformerMixin` and `BaseEstimator`. To subclass these classes, you need to:\n",
    "1. implement the `fit` method, and\n",
    "2. implement the `transform` method.\n",
    "Once you have done that, you can use your custom transformer as part of a `Pipeline` that can leverage all the nice features of scikit-learn (like feature-selection libraries and cross-validation).\n",
    "\n",
    "To get acquainted with the structure of a transformer, it's useful to look at `sklearn` source code. First, you will walk through the source code of the `Binarizer` transformer ([source code](https://github.com/scikit-learn/scikit-learn/blob/7813f7efb/sklearn/preprocessing/data.py#L1789)).\n",
    "\n",
    "The source code is included below. Note, that there is a lot of boiler-plate code, but the *transform* method is the relevant method. (Why does the fit method do nothing?)\n",
    "\n",
    "```\n",
    "class Binarizer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Binarize data (set feature values to 0 or 1) according to a threshold\n",
    "    Values greater than the threshold map to 1, while values less than\n",
    "    or equal to the threshold map to 0. With the default threshold of 0,\n",
    "    only positive values map to 1.\n",
    "    Binarization is a common operation on text count data where the\n",
    "    analyst can decide to only consider the presence or absence of a\n",
    "    feature rather than a quantified number of occurrences for instance.\n",
    "    It can also be used as a pre-processing step for estimators that\n",
    "    consider boolean random variables (e.g. modelled using the Bernoulli\n",
    "    distribution in a Bayesian setting).\n",
    "    Read more in the :ref:`User Guide <preprocessing_binarization>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    threshold : float, optional (0.0 by default)\n",
    "        Feature values below or equal to this are replaced by 0, above it by 1.\n",
    "        Threshold may not be less than 0 for operations on sparse matrices.\n",
    "    copy : boolean, optional, default True\n",
    "        set to False to perform inplace binarization and avoid a copy (if\n",
    "        the input is already a numpy array or a scipy.sparse CSR matrix).\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from sklearn.preprocessing import Binarizer\n",
    "    >>> X = [[ 1., -1.,  2.],\n",
    "    ...      [ 2.,  0.,  0.],\n",
    "    ...      [ 0.,  1., -1.]]\n",
    "    >>> transformer = Binarizer().fit(X)  # fit does nothing.\n",
    "    >>> transformer\n",
    "    Binarizer(copy=True, threshold=0.0)\n",
    "    >>> transformer.transform(X)\n",
    "    array([[1., 0., 1.],\n",
    "           [1., 0., 0.],\n",
    "           [0., 1., 0.]])\n",
    "    Notes\n",
    "    -----\n",
    "    If the input is a sparse matrix, only the non-zero values are subject\n",
    "    to update by the Binarizer class.\n",
    "    This estimator is stateless (besides constructor parameters), the\n",
    "    fit method does nothing but is useful when used in a pipeline.\n",
    "    See also\n",
    "    --------\n",
    "    binarize: Equivalent function without the estimator API.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, threshold=0.0, copy=True):\n",
    "        self.threshold = threshold\n",
    "        self.copy = copy\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Do nothing and return the estimator unchanged\n",
    "        This method is just there to implement the usual API and hence\n",
    "        work in pipelines.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like\n",
    "        \"\"\"\n",
    "        check_array(X, accept_sparse='csr')\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, copy=None):\n",
    "        \"\"\"Binarize each element of X\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape [n_samples, n_features]\n",
    "            The data to binarize, element by element.\n",
    "            scipy.sparse matrices should be in CSR format to avoid an\n",
    "            un-necessary copy.\n",
    "        copy : bool\n",
    "            Copy the input X or not.\n",
    "        \"\"\"\n",
    "        copy = copy if copy is not None else self.copy\n",
    "        return binarize(X, threshold=self.threshold, copy=copy)\n",
    "\n",
    "    def _more_tags(self):\n",
    "        return {'stateless': True}\n",
    "```\n",
    "\n",
    "The relevant (portion of the) function `binarize` from the transform method is here:\n",
    "\n",
    "```\n",
    "def binarize(X, threshold=0.0, copy=True):\n",
    "    ...\n",
    "    cond = X > threshold\n",
    "    not_cond = np.logical_not(cond)\n",
    "    X[cond] = 1\n",
    "    X[not_cond] = 0\n",
    "    return X\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4** \n",
    "\n",
    "As a warm-up, create a transformer that drops the `i`th row of an input dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class ColumnDropper(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, index):\n",
    "        self.index = index\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Does nothing! Stateless!\n",
    "        \"\"\"\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Drops the ith column of X, where i=index\n",
    "        \"\"\"\n",
    "        \n",
    "        return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cd = ColumnDropper(index=3)\n",
    "cd.transform(cars.iloc[:,:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5**\n",
    "\n",
    "Columns that don't have much variation are not very useful for prediction. An extreme case is when a column has only a single value.  Create a \"feature selection\" transformer that drops any columns that don't have a standard-deviation greater than an input threshold.\n",
    "\n",
    "* What needs to be calculated during the fitting process? When is the standard deviation calculated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class LowStdColumnDropper(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, thresh=0):\n",
    "        '''\n",
    "        Drops columns whose standard deviation is less than thresh.\n",
    "        '''\n",
    "        self.thresh = thresh\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        ...\n",
    "        \"\"\"\n",
    "\n",
    "        self.columns_ = ...\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        ...\n",
    "        \"\"\"\n",
    "        \n",
    "        return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lvd = LowStdColumnDropper(thresh=10.0)\n",
    "lvd.fit(cars.select_dtypes('number'))\n",
    "lvd.transform(cars.select_dtypes('number'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-Learn Pipelines\n",
    "\n",
    "Pipelines are ways of chaining transformers and estimators together. \n",
    "* A **Pipeline** object is a sequence of transformers that perhaps end with estimator.\n",
    "* When you call `.fit(X, y)` on a pipeline, the pipeline calls `fit_transform` on each successive transformer in the pipeline, passing the transformed data to the next transformer in the sequence.\n",
    "* A pipeline that consists of a sequence of transformers is itself a transformer.\n",
    "* A pipeline that consists of a sequence of transformers, followed by an estimator, is itself an estimator.\n",
    "    - These observations allow you put pipelines inside of other pipelines!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6** Create a *single* pipeline that consists of a `OneHotEncoder`, followed by a `LowStdColumnDropper`, followed by a `LinearRegression` model. That is, one-hot encode the categorical features, drop the low-variance columns, and use those features to fit a linear regression model. Your threshold for \"small standard deviation\" should be `0.1`. \n",
    "\n",
    "*Remark:* Be sure to pass `sparse=False` into `OneHotEncoder` (or else, make your `LowStdColumnDropper` handle sparse matrices).\n",
    "\n",
    "How many columns did your pipeline drop? (Use the `named_steps` attribute!)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting columns together using `ColumnTransformer`\n",
    "\n",
    "You can run many different transformers in parallel on different subsets of columns using `ColumnTransformer` (see the [docs](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html#sklearn.compose.ColumnTransformer)).\n",
    "\n",
    "One common pattern for using `ColumnTransformer` is to create a transformer pipeline for each kind of data in your dataset and use `ColumnTransformer` to put the transformed features together into a single dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 7**\n",
    "\n",
    "Create a general pipeline that transforms different data-kinds with appropriate generic features (e.g. one-hot-encoder, ordinal-encoder) by filling in the `...` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "quantitative_cols = ...\n",
    "ordinal_cols = ...\n",
    "nominal_cols = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "quantitative_pipeline = Pipeline([...])\n",
    "ordinal_pipeline = Pipeline([...])\n",
    "nominal_pipeline = Pipeline([...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_eng_pipeline = ColumnTransformer([\n",
    "    ('quant', quantitative_pipeline, quantitative_cols),\n",
    "    ('ordin', ordinal_pipeline, ordinal_cols),\n",
    "    ('nomin', nominal_pipeline, nominal_cols)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pl = Pipeline([feature_eng_pipeline, LinearRegression()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 8** \n",
    "\n",
    "Fit the pipeline and predict with it. Check the outputs at each step using `named_steps`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finished? Turn in Question 5 for Grading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
